{
 "metadata": {
  "name": "",
  "signature": "sha256:37f0cfef9695315cd7ecadddd7abe54c9644bfa4483c75dfed8bd180e7e68cd5"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Kernel-Based RL\n",
      "\n",
      "The idea behind KBRL [(Ormoneit 2002)](https://link.springer.com/article/10.1023/A:1017928328829) is to use a set of transition samples in order to estimate the transition and reward functions $P$ and $R$.\n",
      "\n",
      "Since we'll be using kernels to estimate these quantities, we'll assume that 1) $\\mathbb{S} \\subseteq \\mathbb{R}^n$ and 2) $A$ is finite.\n",
      "\n",
      "We will use \n",
      "- $S^a\\equiv \\{(s^a_k,r^a_k,\\hat{s}^a_k)|k=1,...,n_a\\}$ the set of sample transitions when using action $a\\in A$\n",
      "- $k(s,s')$ a kernel function, with some assumptions (see paper for the crunchy details), think of the Gaussian of $\\|s-s'\\|$\n",
      "\n",
      "To obtain normalized values, we will use \n",
      "$$\\kappa^a(s,s_i^a) = \\frac{k(s,s_i^a)}{\\sum_{j=1}^{n_a} k(s,s_j^a)}$$.\n",
      "\n",
      "From these normalized values, we can build an MDP with $n=\\sum n_a$ states (one for each of our samples) and a transition function \n",
      "$$\\hat{P}^a(\\hat{s}_i^b|s)=\\begin{cases}\\kappa^a(s,s_i^b) &\\text{ if $a=b$}\\\\0 &\\text{ otherwise}\\end{cases}$$\n",
      "Similarly we can build the reward function of this new MDP:\n",
      "$$\\hat{R}^a(s,\\hat{s}_i^b)=\\begin{cases}r_i^a &\\text{ if $a=b$}\\\\0 &\\text{ otherwise}\\end{cases}$$\n",
      "\n",
      "Since we now have the transition function, reward function, set of states and set of actions, it is easy to use dynamic programming methods to obtain $\\hat{V}^*$. From this, the action-value function can be determined:\n",
      "$$\\hat{Q}(s,a) = \\sum_{i}^{n_a}\\kappa^a(s,s_i^a)\\left[r_i^a +\\gamma\\hat{V}^*(\\hat{s}_i^a)\\right]$$\n",
      "\n",
      "A major problem of this approach is that solving for $\\hat{V}^*$ can be quite expensive as $n$ grows, since each iteration of a Bellman operator is $O(n^2|A|)$.\n",
      "\n",
      "### Stochastic Factorization\n",
      "\n",
      "A **stochastic matrix** has only non-negative element and its rows sum to 1. These matrices are useful, because they can be easily factored in $P=DK$, where typically $P\\in \\mathbb{R}^{n\\times p}$ is a large matrix, and $D\\in {n\\times m}, K \\in {m \\times p}$ are two thin matrices.\n",
      "\n",
      "When $n=p$, it is possible to define $\\bar{P}=KD$. This matrix has several properties, but most importantly it is also a stochastic matrix.\n",
      "\n",
      "In the MDP case, when $P^a=D^aK^a$ is the transition matrix, $\\bar{P}^a$ can also be thought of a transition matrix between artificial states. There $K^a$ can be thought of the conversion function from $S$ to $\\bar{S}$, and $D^a$ as the transition from $\\bar{S}$ to $S'$ after taking action $a$.\n",
      "\n",
      "### KBSF\n",
      "\n",
      "The idea of KBSF is to use such a factorization to our advantage, and avoid the $n^2$ growth in the number of samples by creating a constant number $m<n$ of artificial states to be used in KBRL. \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These $m$ states are called *representative states*, and it remains an open problem how on to select or compute such states.\n",
      "\n",
      "From these states $\\bar{S}=\\{\\bar{s}_1,\\bar{s}_2,...,\\bar{s}_m\\}$ and the full sample set $S^a=\\{(s_k^a,r_k^a,\\hat{s}^a_k)|k\\in[n_a]\\}\\forall a\\in A$, we can now compute the factorization of the full transition matrix $P=DK$ as:\n",
      "$$D^a:d_{ij}^a =\\kappa(\\hat{s}^a_i,\\bar{s}_j)$$\n",
      "$$K^a:k_{ij}^a =\\kappa(\\bar{s}_i,s_j^a)$$\n",
      "as well as the reward function\n",
      "$$\\bar{r}_i^a=\\sum_j k_{ij}^ar_j^a$$\n",
      "Now we don't need to compute $P$, computing $\\bar{P}^a=K^aD^a$ is enough to solve for $\\bar{Q}$. Intersetingly, in the limite of $n$ and $m$, $\\bar{Q}$ yields the optimal action-value function, and thus the optimal policy."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}